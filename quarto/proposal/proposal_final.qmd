---
title: "High Performance Automated Galaxy Classification for Next Generation Telescopes"
subtitle: "DATA7901 Semester 2, 2025"
author: "Greg Siemon (33222314)"
date: 09/29/2025
date-format: long
execute: 
    echo: false
    warning: false
format: 
    pdf:
        documentclass: scrreprt
        geometry:
            - top=25mm
            - bottom=25mm
            - left=25mm
            - right=25mm
            - heightrounded
        papersize: A4
        linestretch: 1.5
        number-sections: true
        toc: true
        lot: true
        lof: true
        appendix-style: default
        header-includes: 
          - | 
            \usepackage[super]{nth}
bibliography: Data7901.bib
csl: ieee.csl
---
# Introduction

```{python}
#| label: import_packages

import polars as pl
import polars.selectors as cs
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import duckdb


```

By the end of 2026, a new telescope will have generated more data than all astronomical observations combined @boyleThisRevolutionaryNew.  Imaging the entire southern sky every few nights for ten years, the Vera C Rubin telescope will observe data on billions of galaxies @RubinObservatory.  Galaxy data is critical for development and testing of models of galaxy formation and evolution. Galaxy models can further the development of models of the universe improving our understanding of the universe's formation and structure @parryGalaxyMorphologyLCDM2009. One problem in this field is classification of galaxies based on their structure (morphology) which changes as they form, grow and merge with other galaxies. The vast quantity of galaxy data arriving from the Vera C Rubin telescope will unable to classify galaxies using humans, which has been a common method for over a century @hubbleRealmNebulae1982, @lintottGalaxyZoo12011.  It will be virtually impossible to find enough volunteers to classify all of the galaxies that are discovered by new generation telescopes.  Astronomers and cosmologists require highly performant, efficient and accurate computerised methods for galaxy classification. This proposal outlines a project that will evaluate the performance of a range of machine learning methods that will be used for classifying galaxies, selecting the the most efficient method or combination of methods based on a set of metrics. The best machine learning methods will be assembled into an automated, high performance galaxy data processing pipeline.  This work will use data from three main sources, the Sloan Digital Sky Survey (SDSS), the Dark Energy Spectroscopic Instrument (DESI) Legacy Survey and the Galaxy Zoo project @collaborationNineteenthDataRelease2025, @deyOverviewDESILegacy2019, @lintottGalaxyZooMorphologies2008.  Together these three datasets will encompass the typical measurements obtained from galaxy surveys and previous classification projects.  This proposal outlines: the state of the art for galaxy classification methods and the datasets that are available; why previous approaches are no longer suitable; specific project objectives and; a brief outline of the proposed solution. The current state of Galaxy classification, including methods, are discussed in the next section.

# Current situation/methods

## Galaxy classification background

A significant debate occurred in the 1920s as to the size of the universe and whether 'spiral nebulae' were part of our own galaxy.  Through observations, Edwin Hubble resolved this debate by showing that 'Spiral Nebulae' were separate galaxies in their own right and significantly further away than anyone had previously thought @GreatDebateAstronomy2025. Hubble developed a classification system for galaxies as shown in @fig-tuning_fork. The system classifies galaxies into two main types, Elliptical (E) and Spiral (S). Elliptical galaxies are classified by how stretched a galaxy is from nearly spherical (E0) to very elongated (E7). Spiral galaxies are subdivided into normal spirals (S) and barred spirals (SB) depending on whether they have a central bar structure. Both spiral galaxies classes are further subdivided based on how tightly wound the spiral arms are, from tightly wound (a) to loosely wound (c) @information.eso.orgHubbleTuningForka . The galaxies are represented in a tuning fork arrangement because Hubble thought that elliptical galaxies evolved into spiral galaxies.  While elliptical galaxies do not evolve into spiral galaxies, the classification system is still widely used. For this project I will focus on classifying galaxies using a simpler system based on the Galaxy Zoo project (@fig-galaxy_zoo_morphology) that determines if a galaxy is Elliptical, Spiral (with a clock-wise or anti-clockwise rotation), a Spiral Galaxy (seen side on) or whether it appears that two galaxies are merging @lintottGalaxyZoo12011.  A fallback category of 'Don't Know' is also used when the image is too unclear to classify.  The Galaxy Zoo classification system will be further simplified to just three categories, Elliptical, Spiral and Other.  This brings us to one of the key issues in Galaxy classification, how are galaxies classified now?

![Edwin Hubble's galaxy classification system @information.eso.orgHubbleTuningForka](images/heic9902o.jpg){#fig-tuning_fork}

![Galaxy Zoo morphology @lintottGalaxyZoo12011](images/gz1_classes_table.png){#fig-galaxy_zoo_morphology}

## Galaxy Classification Methods

It is easy for a human to classify a galaxy from its shape, if we can obtain a clear photograph. That is what Edwin Hubble and his group did and it remains a common method of classification today. The number of galaxy images reached a point where there were not enough skilled humans to classify them. In 2007, the Galaxy Zoo initiative commenced as a Citizen Science project asking members of the public to classify images of galaxies collected from telescopes and displayed on a website.  The project demonstrated that non-experts can classify galaxies as well as professional astronomers and that it can be done at scale @lintottGalaxyZooMorphologies2008.  Over 100 000 volunteers were involved, classifying over one million galaxies @lintottGalaxyZoo12011.  The Galaxy Zoo classifications for over 475 000 galaxies are one of the datasets available to this project. However, the sheer quantity of data about to arrive from the Vera C Rubin telescope means that even a Galaxy Zoo style approach will not be sufficient. Either astronomers need to find a way to massively scale up the number of human classifiers (beyond the scope of this project) or automate as much of the galaxy classification process as possible. This project is focused on automating galaxy classification using machine learning methods. Before discussing the relevant machine learning methods, it is important to understand the datasets that are available to this project as the data will influence the choice of methods.

## Datasets for Galaxy Classification

### Galaxy Images

Three main data sources are generally available from galaxy surveys, images (already briefly discussed), spectra and tabulated data. I mentioned above that we can classify galaxies if clear images are available. Obtaining clear images, requires the use of a large telescope and a sensitive camera. The process of obtaining a clear image is significantly more complicated than just pointing the telescope at the sky and triggering the camera @surveyDataReleaseDescription2023.  The most important aspect of the imaging process is that astronomy cameras only take black and white images. To obtain a colour image, multiple photographs are taken using filters eg red, green and blue but often different infrared or ultraviolet wavelengths are required. The filters pass light in a narrow band centred around a particular wavelength of light. Astronomer's refer to each band using a letter (sometimes a letter and number). For example, the DESI Legacy survey images use the bands g (green-blue light), r (red) and z (near-infrared). The images may be combined to create a colour or often a false colour image if the images are not made up of red, green and blue filter photographs. An image directly out of the telescope contains multiple objects such as stars and galaxies. An image processing pipeline is use to identify and isolate the individual objects in the image to output a separate image for each object.  The images are generally quite small and the ones in this project are 256 x 256 pixels. The specialised camera pixels are designed in a way that they can precisely measure the brightness of an object and therefore the amount of light it is emitting (if we know how far away it is) @bessellStandardPhotometricSystems2005. We've discussed that photographs use filters to image objects in certain light bands in a grid of pixels. Spectroscopy is the technique of measuring the amount of light emitted by any object across thousands of extremely narrow light bands at one specific point and is discussed below @morisonIntroductionAstronomyCosmology2008. The DESI Legacy Survey @provides images of approxmately 475 000 galaxies @deyOverviewDESILegacy2019.

### Galaxy Spectra
A spectrum of a galaxy is obtained by splitting the light from the galaxy into its component wavelengths (similar to how a rainbow forms). Light is collected by the telescope and directed to a plate containing a single optical fibre for each target object. Each plate has to be configured specifically for observing a particular section of the sky @morisonIntroductionAstronomyCosmology2008. The spectrum encodes information about the composition of the matter that produced the light @tojeiroRecoveringGalaxyStar2007. Spectral data is extremely costly relative to imaging data as it requires a large amount of observing time (up to 1000 times more than for an image) in addition to the need to construct the optical fibre plates for each observation @wuPredictingGalaxySpectra2020. For this reason, and also the fact that the Vera C. Rubin telescope will not have any spectrographic capabilities, this project will not rely on any spectral data.  Photometric data will be used instead to estimate the redshift of galaxies if necessary. The SDSS dataset includes spectra for the same 475 000 galaxies extracted from the DESI Legacy Survey dataset @collaborationNineteenthDataRelease2025, @deyOverviewDESILegacy2019. 

### Tabulated Galaxy Datasets
The Galaxy Zoo dataset is an important tabulated dataset.  It contains the number of votes from project volunteers for each class as shown in @fig-galaxy_zoo_morphology @lintottGalaxyZoo12011, @lintottGalaxyZooMorphologies2008.  Having the class votes for each galaxy means it is not only possible to determine the most likely class but also the collective confidence in that classification by the volunteers. The majority class for each galaxy in the Galaxy Zoo dataset will be the source of truth when training machine learning models.  The SDSS and DESI Legacy Survey datasets also contain tabulated data derived from the images and spectra that may be useful as features for machine learning models which will be discussed shortly but first we need to discuss the concept of redshift.

Redshift is therefore a complicating factor in galaxy classification and needs to be considered when designing an automated classification system. Edwin Hubble, through his observation of galaxies, showed that every galaxy appeared to moving away from Earth and that galaxies that were further away were receding faster @hubbleRealmNebulae1982. Hubble's observations were evidence that the universe was expanding. The rate at which a galaxy is receding from Earth shifts the light we observe toward the red part of the electromagnetic spectrum i.e. the spectrum is redshifted. This means that two otherwise similar galaxies that are at different distances from the Earth will have different spectra and also differences in their images, the one further away will be 'redder'.  Now that we have some understanding of the available data, we can consider the machine learning methods that are suitable for galaxy classification.

## Machine Learning Methods for Galaxy Classification

Machine learning methods such as deep learning algorithms lend themselves to the images and spectra datasets while traditional machine learning methods such as Random Forests or Nearest neighbour models are more suited to the tabulated data @LindholmAndreas2022Ml:a.  Each of these methods have their strengths and weaknesses. Deep Learning models such as Convolutional Neural Networks (CNNs) are excellent at using images for classification tasks particularly because they can cope with variation in the position, size and orientation of objects in an image.  They have been successfully used to classify galaxies using telescope images @cavanaghMorphologicalClassificationGalaxies2021. Spectral data is often processed using another variant of Deep Learning, a Long-Short Term Memory algorithm (LSTM). Both deep learning models can process the data with limited preparation. However, Deep Learning models are computationally intensive to train and often require specialised computer processors such as Graphics Processing Units (GPUs) are are not only expensive also consume significant amounts of power.  Random Forests on the other hand are much less computationally intensive to train and can run efficiently on standard computers but significant processing is required to calculate and tabulate the data before a Random Forest can work with it.  It is not clear which of these methods, or even combination of methods, is best suited to automated galaxy classification and this is the reason for this project.  In addition there are complicating factors in the datasets that need to be considered.  This will be discussed below.

# Problem identification

In the previous section we discussed that if we could obtain a clear image of a galaxy then we can classify it.  The problem is obtaining a clear image of a galaxy is not that easy.  Galaxies that are very far away, appear as small blobs in images and images of two similar galaxies can still appear quite different due to the red-shift.  Simply throwing images at a machine learning algorithm with no thought or provision of contextual data is unlikely to result in success. Further work is required to understand what data is required for effective galaxy classification. While determining what data is important, some consideration of the Vera C. Rubin Observatory's design needs to be considered to understand what data will be available in the future.

The Vera C. Rubin Observatory will not have any spectrographic capability and will generate images and image based tabular data only.  While existing spectrographic datasets can be used to supplement data from the Vera C. Rubin Observatory, this is only possible for galaxies that have already been observed.  Given that the new telescope will be discovering many new galaxies, it is unwise to rely on spectrographic data.  Further work is required to determine if the spectrographic data is required or if images and image derived data is sufficient.  Even if the right data is available, understanding how it can be efficiently acquired and processed is another important consideration.

The datasets available for this project are large and some thought is required on how to process the daily data stream from the Vera C. Rubin Observatory.  The combined size of the datasets in this project are over 90GB and took over 12 hours to download before any processing could be done.  The new telescope is expected to generate 20TB of data per night @RubinObservatory and downloading, storing and processing this data will be a significant challenge.  The design of an automated data processing pipeline that can efficiently acquire, process and store the results is an important aspect of this project.

# High-performance automated galaxy classification project objectives 

This project aims to:

* Determine the necessary data requirements for accurate galaxy morphology classification e.g. images, spectra, tabulated data or a combination of these

* Determine a set of metrics for comparing the performance of machine learning methods for galaxy morphology classification e.g. classification accuracy, speed (galaxies classified per hour per computing unit) and 'computational effort'.

* Evaluate the performance existing machine learning methods using the three datasets to determine 

* Determine the most efficient galaxy classification method or combination of methods and the most important measurements that the machine learning models rely on from the datasets

* Design and test an automated galaxy classification data pipeline that acquires new galaxy images, classifies them and inserts the results into a database

# Proposed solution

To meet the objectives of this project I am proposing a series of experiments is carried out.  The experiments will train and evaluate several types of machine learning models using the datasets and methods that have been discussed above.  In each experiment, galaxy classification models will be trained using a subset of the available data, e.g. images only, images plus select tabulated data, etc.  Each classifier will be evaluated using a set of metrics that include classification accuracy, classification speed and computational effort (to prepare the data and train the model). The results of the experiments will be analysed to determine the optimal data requirements and the most efficient method or combination of methods. The best models will be incorporated into an automated processing pipeline which will be evaluated for its effectiveness and performance. The outcomes of this work will provide valuable insights into the data requirements and machine learning methods that are best suited to automated galaxy classification.  Additionally, the project will quantify the computational resources required to process a simulated datastream from telescopes such as the Vera C. Rubin Observatory. This knowledge will ultimately lead to high performance, automated galaxy classification enabling astronomers and cosmologists to better understand how galaxies form and evolve and ultimately improve our understanding of the universe.

# Method for Developing the Program


# Resources

The resources required for this project are computer datasets, computer hardware and software. All of these are available now. The complete datasets have already been acquired and downloaded and have been used in the preparation of this proposal. 

## Computer hardware requirements
The primary computer hardware requirement is access to a fast computer with a GPU (for training deep learning models), at least 64 GB of memory and at least 150 GB of available disk storage to hold the datasets (90GB) and intermediate results. I own and will make available to the project an Apple Mac Studio (M1 Ultra with integrated GPU, 128GB memory and 8TB of storage), a Apple MacBook Pro (M3 Max processor with integrated GPU, 128GB memory and 8TB of storage) and a HP laptop (Intel Core i9 and Nvidia RTX4000 GPU, 64GB memory and 2TB storage). If these are found to be insufficient, access to The University of Queensland's High-Performance Computing resources may be required. A request for access has been made through the project's supervisors but has not been approved at the time of writing this proposal. If access is not granted, the project will be limited to using smaller datasets and simpler models that can run on available hardware. This may impact the quality of the results. Software selection is not dependent on the hardware and will be discussed below.

## Computer software requirements

Python is the primary programming language used for this project @vanrossumPython3Reference2009. Python is widely used in the astronomy, data science and machine learning communities and there are a large number of open source libraries available for data processing, machine learning and visualisation @robitailleAstropyCommunityPython2013, @raschkaMachineLearningPython2020.  The primary libraries that will be used include Polars for data processing, Scikit-learn for traditional machine learning methods such as nearest neighbour or random forest algorithms, PyTorch for deep learning methods and Seaborn for visualisation @polarsdevelopersPolarsBlazinglyFast2025, @pedregosaScikitlearnMachineLearning2011, @paszkePyTorchImperativeStyle2019, @waskomSeabornStatisticalData2021 .  Jupyter Notebooks will be used as the primary development environment [@kluyverJupyterNotebooksPublishing2016]. All of these Python libraries are open source and freely available. Github will be used for managing the source code for the project via a dedicated repository @githubGitHub2025. Software will also be required to document the project's development and outcomes. It is important to be able to generate reports and presentations so that the documents can be easily reproduced by any one with access to the project's materials. 

The ability of anyone to easily replicate results and reports is embodied in the concept of Reproducible Research and its principles will be followed in this project @alstonBeginnersGuideConducting2021.  Quarto is a open source tool for creating documents, reports and presentations that can integrate code, results (including images and tables generated by code) and narrative text @allaireQuarto2025.  Quarto has been used for this document and will be used for all subsequent reports and presentations. The combination of Jupyter Notebooks, Python, Github and Quarto provides a powerful and flexible environment for developing, documenting and sharing the results of this project. All of the software and reports must be backed up to ensure that no work is lost.

Backing up work is a critical activity to ensure that work is not lost and that project milestones are able to be met in the event that a computer fails or important documents are deleted or corrupted.  I have a comprehensive backup strategy on my Apple computers that stores local Apple Time Machine backups on network attached storage (NAS) at my home and offsite backups to Backblaze and Amazon S3 Deep Glacier cloud services. Time Machine and Backblaze store all changes to files allowing recovery of previous versions if necessary and data can be restored immediately.  Amazon S3 Deep Glacier is designed for long term archival of data and is not suitable for frequent restores but is a very low cost solution for long term storage of important data.  The combination of these three backup methods plus source code control via Github provides a robust and reliable backup solution for the project.    

# Schedule
Outline the expected time frames for the semester perhaps include an Gantt chart.

![Project Gantt chart](images/galaxy_classification_timeline.pdf){#fig-project_gantt_chart width=150%}

# Costs
This project does not incur any direct financial costs.  The datasets are all freely available and open source software will be used for the data processing, machine learning evaluation and reporting.  The computer hardware required for the project is already owned by me and available. The software used for cloud backups an the associated storage costs are less than 20 cents per month which I am happy to pay.  The main cost of the project is my time.  I am budgeting approximately 130 hours during Semester 1 2026 to complete this project.  The project supervisors time is an additional cost, I am budgeting approximately 1 hour per week for supervision and advice. 

# References

::: {#refs}
:::

# Appendices
Include detailed charts, tables, calculations, visualisations of data (besides the
main ones used up front in the proposal to highlight the key findings or
explanations).

```{python}

```
