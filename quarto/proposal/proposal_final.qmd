---
title: "High-Performance Automated Galaxy Classification for Next Generation Telescopes"
subtitle: "DATA7901 Semester 2, 2025"
author: "Greg Siemon (33222314)"
date: "2025-09-26"
date-format: long
execute: 
    echo: false
    warning: false
format: 
    pdf:
        documentclass: scrreprt
        geometry:
            - top=25mm
            - bottom=25mm
            - left=25mm
            - right=25mm
            - heightrounded
        papersize: A4
        linestretch: 1.5
        number-sections: true
        toc: true
        lot: true
        lof: true
        appendix-style: default
        include-in-header:
            - file: add_logo_to_title_page.tex
        header-includes: 
          - | 
            \usepackage[super]{nth}
        include-before-body: 
            text: |
                \pagenumbering{roman}
        maketitle: false
bibliography: Data7901.bib
csl: ieee.csl
---
\clearpage
\pagenumbering{arabic}
# Introduction

```{python}
#| label: import_packages

import polars as pl
import polars.selectors as cs
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import duckdb
```

By the end of 2026, a new telescope will have generated more data than all astronomical observations combined @boyleThisRevolutionaryNew.  Imaging the entire southern sky every few nights for ten years, the Vera C Rubin telescope will observe billions of galaxies @RubinObservatory.  Galaxy data is critical for the development and testing models of galaxy formation and evolution @javanmardiAnisotropyAllskyDistribution2017. Galaxy models can further the development of models of the universe improving our understanding of the universe's formation and structure @parryGalaxyMorphologyLCDM2009, @deruelleLambdaCDMModelHot2018. One problem in this field is the classification of galaxies based on their structure (morphology) which changes as they form, grow and merge with other galaxies. The vast quantity of galaxy data arriving from the Vera C Rubin telescope mean that classification projects will not be able to recruit enough humans, who have done the majority of the work for over a century @hubbleRealmNebulae1982, @lintottGalaxyZoo12011.  Astronomers and cosmologists now require highly performant, efficient and accurate computerised methods for galaxy classification. This proposal outlines a project that will evaluate the performance of a range of machine learning methods to classify galaxies, selecting the the most efficient method or combination of methods based on a set of metrics. The best machine learning methods will be assembled into an automated, high performance galaxy data processing pipeline.  This work will use data from three main sources, the Sloan Digital Sky Survey (SDSS), the Dark Energy Spectroscopic Instrument (DESI) Legacy Survey and the Galaxy Zoo project @collaborationNineteenthDataRelease2025, @deyOverviewDESILegacy2019, @lintottGalaxyZooMorphologies2008.  Together these three datasets will encompass the typical measurements obtained from galaxy surveys and previous classification projects, images, spectra and tablulated data.  This proposal outlines: the state of the art for galaxy classification methods and the datasets that are available; why previous approaches are no longer suitable; specific project objectives and; a brief outline of the proposed solution. The current state of Galaxy classification, including methods, are discussed in the next section.

# Current situation/methods

## Galaxy classification background

A significant debate occurred in the 1920s as to the size of the universe and whether 'spiral nebulae' were part of our own galaxy.  Through observations, Edwin Hubble resolved this debate by showing that 'Spiral Nebulae' were separate galaxies in their own right and significantly further away than anyone had previously thought @GreatDebateAstronomy2025. Hubble developed a classification system for galaxies as shown in @fig-tuning_fork. The system classifies galaxies into two main types, Elliptical (E) and Spiral (S). Elliptical galaxies are classified by how stretched a galaxy is from nearly spherical (E0) to very elongated (E7). Spiral galaxies are subdivided into normal spirals (S) and barred spirals (SB) depending on whether they have a central bar structure. Both spiral galaxies classes are further subdivided based on how tightly wound the spiral arms are, from tightly wound (a) to loosely wound (c) @information.eso.orgHubbleTuningForka . The galaxies are represented in a tuning fork arrangement because Hubble thought that elliptical galaxies evolved into spiral galaxies.  While elliptical galaxies do not evolve into spiral galaxies, the classification system is still widely used. For this project I will focus on classifying galaxies using a simpler system based on the Galaxy Zoo project (@fig-galaxy_zoo_morphology) that determines if a galaxy is Elliptical, Spiral (with a clock-wise or anti-clockwise rotation), a Spiral Galaxy (seen side on) or whether it appears that two galaxies are merging @lintottGalaxyZoo12011.  A fallback category of 'Don't Know' is also used when the image is too unclear to classify.  The Galaxy Zoo classification system will be further simplified to just three categories, Elliptical, Spiral and Other.  This brings us to one of the key issues in Galaxy classification, how are galaxies classified now?

![Edwin Hubble's galaxy classification system @information.eso.orgHubbleTuningForka](images/heic9902o.jpg){#fig-tuning_fork}

![Galaxy Zoo morphology @lintottGalaxyZoo12011](images/gz1_classes_table.png){#fig-galaxy_zoo_morphology}

## Galaxy Classification Methods

It is generally easy for a human to classify a galaxy from its shape, if we can obtain a clear photograph. That is what Edwin Hubble and his group did and it remains a common method of classification today. The number of galaxy images reached a point where there were not enough skilled humans to classify them. In 2007, the Galaxy Zoo initiative commenced as a Citizen Science project asking members of the public to classify images of galaxies collected from telescopes and displayed on a website.  The project demonstrated that non-experts can classify galaxies as well as professional astronomers and that it can be done at scale @lintottGalaxyZooMorphologies2008.  Over 100 000 volunteers were involved, classifying over one million galaxies @lintottGalaxyZoo12011.  The Galaxy Zoo classifications for over 475 000 galaxies are one of the datasets available to this project @collaborationNineteenthDataRelease2025. However, the sheer quantity of data about to arrive from the Vera C Rubin telescope means that even a Galaxy Zoo style approach will not be sufficient. Either astronomers need to find a way to massively scale up the number of human classifiers which is beyond the scope of this project or automate as much of the galaxy classification process as possible. This project is focused on automating galaxy classification using machine learning methods. Before discussing the relevant machine learning approaches, it is important to understand the datasets that are available to this project as the data will influence the choice of methods.

## Datasets for Galaxy Classification

### Galaxy Images

Three main data sources are generally available for galaxy classification from galaxy surveys, images, spectra and tabulated data. I mentioned above that we can classify galaxies if clear images are available. Obtaining good quality images, requires the use of a large telescope to collect a lot of light from the target galaxy and a sensitive camera. The process of obtaining a clear image is significantly more complicated than just pointing the telescope at the sky and triggering the camera @surveyDataReleaseDescription2023.  The most important aspect of the imaging process is that astronomy cameras only take black and white images. A colour image is assembled from multiple photographs taken through filters eg red, green and blue but often different infrared or ultraviolet wavelengths are required. The filters pass light in a narrow band centred around a particular wavelength of light. Astronomer's refer to each band using a letter (sometimes a letter and number). For example, the DESI Legacy survey images use the bands g (green-blue light), r (red) and z (near-infrared). The images may be combined to create a colour or often a false colour image if the images are not made up of red, green and blue filter photographs. An image directly out of the camera contains multiple objects such as stars and galaxies and also asteroids and satellites. An automated image processing pipeline is use to identify and isolate the individual objects in the image to output a separate image for each object @surveyDataReleaseDescription2023.  The images are generally quite small e.g. the ones in this project are 256 x 256 pixels with three 'colour' channels per pixel. The specialised camera pixels are designed in a way that they can precisely measure the brightness of an object and therefore the amount of light it is emitting (if we know how far away it is) @bessellStandardPhotometricSystems2005. This is important as objects of the same inherent brightness appear dimmer as the distance from the observer increases. Data acquired in this way is known as photometric data. The DESI Legacy Survey provides images of approxmately 475 000 galaxies @deyOverviewDESILegacy2019 and the SDSS dataset contains photometric measurements @ollaborationNineteenthDataRelease2025.  I'll now discuss the second type of data available for galaxy classification, spectra.

### Galaxy Spectra
We've discussed that photographs use filters to image objects in certain light bands over a grid of pixels. Spectroscopy is the technique of measuring the amount of light emitted by an object across thousands of extremely narrow light bands at a single location @morisonIntroductionAstronomyCosmology2008. A galaxy's spectrum is obtained by splitting the light from the galaxy into its component wavelengths (similar to a rainbow). Light is collected by the telescope and directed onto a plate containing one optical fibre for each target object. Each plate has to be configured specifically for observing a particular section of the sky @morisonIntroductionAstronomyCosmology2008. The spectrum encodes information about the composition of the matter that produced the light @tojeiroRecoveringGalaxyStar2007. The composition of the galaxy indicates what types of stars are present and can be used to infer the galaxy classification.  Unfortunately, Spectral data is extremely costly relative to imaging data as it requires a large amount of observing time (up to 1000 times more than required for an image) in addition to the need to construct the optical fibre plates for each observation run @wuPredictingGalaxySpectra2020. The Vera C. Rubin telescope will not have any spectrographic capabilities and therefore this project will not rely directly on any spectral data.  Photometric data will be used instead to estimate the redshift of galaxies if necessary. The SDSS dataset includes spectra for the same 475 000 galaxies extracted from the DESI Legacy Survey dataset @collaborationNineteenthDataRelease2025, @deyOverviewDESILegacy2019. 

### Tabulated Galaxy Datasets
The Galaxy Zoo dataset is an important tabulated dataset.  It contains the number of votes from project volunteers for each class as shown in @fig-galaxy_zoo_morphology @lintottGalaxyZoo12011, @lintottGalaxyZooMorphologies2008.  Having the class votes for each galaxy means it is not only possible to determine the most likely class but also the collective confidence in that classification by the volunteers. @fig-plot-class-distribution and @fig-plot-difficulty-distribution show the class and classification distributions using a simplified set of categories chosen to try to quickly get an understanding of the dataset and how it might be used in this project. The majority class for each galaxy in the Galaxy Zoo dataset will be the source of truth when training machine learning models.  Three example images that have been classified as spiral, eliptical and other (the simple set of classes this classification project will start with) are shown in @fig-example-galaxy-images. The SDSS and DESI Legacy Survey datasets also contain tabulated data derived from the images and spectra that may be useful as features for machine learning models which will be discussed shortly but first we need to discuss the concept of redshift.

::: {#fig-example-galaxy-images layout-ncol=3}

![Eliptical](images/1237667917030228104.jpeg){#fig-eliptical}

![Spiral](images/1237667967496683808.jpeg){#fig-spiral}

![Other](images/1237649918432116906.jpeg){#fig-other}

Example of DESI Legacy Survey images of galaxies classified as (a) Eliptical, (b) Spiral and (c) Other by the Galaxy Zoo project as found in the SDSS dataset @deyOverviewDESILegacy2019, @lintottGalaxyZoo12011, @collaborationNineteenthDataRelease2025.
:::

```{python}
#| label: load_data

con = duckdb.connect(database="../../input/database/sdss.duckdb", read_only=False)

df = con.execute(
    """SELECT DISTINCT
    obj.objid, 
    zoo.*, 
    photoz.*,
    CASE
        WHEN p_el >= p_cs AND p_el >= p_other then 'elliptical'
        WHEN p_cs >= p_el AND p_cs >= p_other then 'spiral'
        ELSE 'other'
    END AS class,
    CASE
        WHEN GREATEST(p_el, p_cs, p_other) >= 0.75 then 'easy'
        WHEN GREATEST(p_el, p_cs, p_other) < 0.5 then 'hard'
        ELSE 'moderate'
    END AS difficulty
    FROM objects AS obj
    JOIN zoo ON obj.objid = zoo.objid
    JOIN photoz ON obj.objid = photoz.objid
    WHERE 
    nvote_tot IS NOT NULL AND
    z IS NOT NULL AND
    Z > 0 AND Z <=0.2
 """
).pl()

```

```{python}
#| label: fig-plot-class-distribution
#| fig-cap: Distribution of classes in the Galaxy Zoo dataset - simplified to three classes @lintottGalaxyZoo12011
order_class = ["elliptical", "spiral", "other"]

plt.xlabel("Galaxy Class")
plt.ylabel("Number of Galaxies")

sns.countplot(x="class", data=df, order=order_class)
```

```{python}
# | label: fig-plot-difficulty-distribution
# | fig-cap: Distribution of classification confidence in the Galaxy Zoo dataset @lintottGalaxyZoo12011
order_difficulty = ["easy", "moderate", "hard"]

plt.xlabel("Classification Confidence")
plt.ylabel("Number of Galaxies")

sns.countplot(x="difficulty", data=df, order=order_difficulty)
```

Redshift is therefore a complicating factor in galaxy classification and needs to be considered when designing an automated classification system. Edwin Hubble, through his observation of galaxies, showed that every galaxy appeared to moving away from Earth and that galaxies that were further away were receding faster @hubbleRealmNebulae1982. Hubble's observations were evidence that the universe was expanding. The rate at which a galaxy is receding from Earth shifts the light we observe toward the red part of the electromagnetic spectrum i.e. the spectrum is redshifted. This means that two otherwise similar galaxies that are at different distances from the Earth will have different spectra and also differences in their images, the one further away will be 'redder'.  A good example of how redshift affects spectra can been seen in @fig-redshift-vega-spectra which shows the spectra (blue) of a the star Vega if we were to observe the star Vega from close by (redshift = 0) @235RegressionPhotometric. Most of the light being emitted would be collected in an image using a u (utraviolet) or a g (green) filter.  It the star is far away (redshift = 0.8), the spectra would be measured as the red line. The light emitted from the star has been shifted toward the red end of the spectrum by the expansion of the space between the star and the Earth and the light received by a camera would now see the peak light emissions through an i (infrared) filter. Now that we have some understanding of the available data, we can consider the machine learning methods that are suitable for galaxy classification.

![Calculated spectrum of the star Vega at different redshifts. Corresponding image filter bands are indicated in grey. @235RegressionPhotometric](images/plot_sdss_filters_21.png){#fig-redshift-vega-spectra}

## Machine Learning Methods for Galaxy Classification

Machine learning methods such as deep learning algorithms lend themselves to the images and spectra datasets while traditional machine learning methods such as Random Forests or Nearest neighbour models are more suited to the tabulated data @LindholmAndreas2022Ml:a.  Each of these methods have their strengths and weaknesses. Deep Learning models such as Convolutional Neural Networks (CNNs) are excellent at using images for classification tasks particularly because they can cope with variation in the position, size and orientation of objects in an image.  They have been successfully used to classify galaxies using telescope images @cavanaghMorphologicalClassificationGalaxies2021.  One of the key parameters in a CNN is the number of processing layers in the model @LindholmAndreas2022Ml:a.  More layers generally means a more powerful model but it was found that after a certain number of layers, the model performance degrades.  Residual Networks (ResNets) were developed to overcome this problem by adding 'skip connections' that allow the model to have a large number of layers without the performance issues. The ResNet50 model (with 50 layers) has been successfully used for galaxy classification @heClassificationGalaxyMorphology2023a. Deep learning models such as CNNs and ResNets have an additional advantage in that pretrained models are available that have been trained on very large image datasets @Resnet50TorchvisionMain.  Pretrained models can be fine-tuned for a specific task using a much smaller dataset that would otherwise be required to train an effective model from scratch. This process is called transfer learning and has been successfully used for galaxy classification @andrewGalaxyClassificationUsing2023. A proof of concept image classifier has been trained using the ResNet50 model and is discussed in the Appendix. Deep learning models may also lend themselves to can spectral data given the large number of measurements in each one but further work is required to understand which ones may be useful. The main issue with deep learning models is they are computationally intensive to train and often require specialised computer processors such as Graphics Processing Units (GPUs) are are not only expensive to purchase also consume significant amounts of electricity during operation. Traditional machine learning models such as Random Forests on the other hand are much less computationally intensive to train and can run efficiently on standard computers but significant processing may be required to calculate and tabulate the necessary data (feature selection) before a model can be trained.  It is not clear which of these methods, or even combination of methods, is best suited to automated galaxy classification and this is the reason for this project. This is why this project is important.  However, there is a signficant issue with machine learning and deep learning methods that needs to be considered, these algorithms can only classify objects into the classes they have been asked to find in the training data.


The models described above are examples of supervised machine learning where a model is trained to categorise objects based on a set of predefined classes (labels). The model learns to associate features in the data with the labels. If the model comes across data that doesn't fit into one of the predefined classes, it will still try to classify it using one of the labels it has been trained on.  This is a significant issue as there may be objects in the data that should have a new label. During the Galaxy Zoo project, volunteers uncovered a type of object that appears to be a new type of galaxy that were named 'Green Peas' due to their resemblance to a pea @cardamoneGalaxyZooGreen2009.  This is one area where human classifiers have an advantage over machine learning models.  Astronomers and cosmologists are interested in finding objects like 'Green Peas' that do not fit into existing classifications as they may provide new insights into how the universe works. This brings us to the concept of unsupervised machine learning models where the model is not given any information about the classes in the data and is instead trying to group similar objects together based on their features @LindholmAndreas2022Ml:a.

Unsupervised learning methods such as clustering algorithms couple with methods to reduce the number of dimensions present in the datasets may be useful for identifying 'outliers' in the data that do not fit well with existing classes.  Outliers may be either data errors or new types of objects.


In addition there are complicating factors in the datasets that need to be considered.  This will be discussed below.

# Problem identification

In the previous section we discussed that if we could obtain a clear image of a galaxy then we can classify it.  The problem is obtaining a clear image of a galaxy is not that easy.  Galaxies that are very far away, appear as small blobs in images and images of two similar galaxies can still appear quite different due to the red-shift.  Simply throwing images at a machine learning algorithm with no thought or provision of contextual data is unlikely to result in success. Further work is required to understand what data is required for effective galaxy classification. While determining what data is important, the Vera C. Rubin Observatory's design needs to be considered to understand what data will be available in the future.

The Vera C. Rubin Observatory will not have any spectrographic capability and will generate images and image based tabular data only.  While existing spectrographic datasets can be used to supplement data from the Vera C. Rubin Observatory, this is only possible for galaxies that have already been observed.  Given that the new telescope will be discovering many new galaxies, it is unwise to rely on spectrographic data.  Further work is required to determine if the spectrographic data is required or if images and image derived data is sufficient.  Even if the right data is available, understanding how it can be efficiently acquired and processed is another important consideration.

The datasets available for this project are large and some thought is required on how to process the daily data stream from the Vera C. Rubin Observatory.  The combined size of the datasets in this project are over 90GB and took over 12 hours to download before any processing could be done.  The new telescope is expected to generate 20TB of data per night @RubinObservatory and downloading, storing and processing this data will be a significant challenge.  The design of an automated data processing pipeline that can efficiently acquire, process and store the results is an important aspect of this project.

# High-performance automated galaxy classification project objectives 

This project aims to:

* Determine the necessary data requirements for accurate galaxy morphology classification e.g. images, spectra, tabulated data or a combination of these

* Determine a set of metrics for comparing the performance of machine learning methods for galaxy morphology classification e.g. classification accuracy, speed (galaxies classified per hour per computing unit) and 'computational effort'.

* Evaluate the performance existing machine learning methods using the three datasets to determine 

* Determine the most efficient galaxy classification method or combination of methods and the most important measurements that the machine learning models rely on from the datasets

* Design and test an automated galaxy classification data pipeline that acquires new galaxy images, classifies them and inserts the results into a database

# Proposed solution

To meet the objectives of this project I am proposing a series of experiments is carried out.  The experiments will train and evaluate several types of machine learning models using the datasets and methods that have been discussed above.  In each experiment, galaxy classification models will be trained using a subset of the available data, e.g. images only, images plus select tabulated data, etc.  Each classifier will be evaluated using a set of metrics that include classification accuracy, classification speed and computational effort (to prepare the data and train the model). The results of the experiments will be analysed to determine the optimal data requirements and the most efficient method or combination of methods. The best models will be incorporated into an automated processing pipeline which will be evaluated for its effectiveness and performance. The outcomes of this work will provide valuable insights into the data requirements and machine learning methods that are best suited to automated galaxy classification.  Additionally, the project will quantify the computational resources required to process a simulated datastream from telescopes such as the Vera C. Rubin Observatory. This knowledge will ultimately lead to high performance, automated galaxy classification enabling astronomers and cosmologists to better understand how galaxies form and evolve and ultimately improve our understanding of the universe.

# Method for Developing the Program


# Resources

The resources required for this project are computer datasets, computer hardware and software. All of these are available now. The complete datasets have already been acquired and downloaded and have been used in the preparation of this proposal. 

## Computer hardware resource requirements
The primary computer hardware requirement is access to a fast computer with a GPU (for training deep learning models), at least 64 GB of memory and at least 150 GB of available disk storage to hold the datasets (90GB) and intermediate results. I own and will make available to the project an Apple Mac Studio (M1 Ultra with integrated GPU, 128GB memory and 8TB of storage), a Apple MacBook Pro (M3 Max processor with integrated GPU, 128GB memory and 8TB of storage) and a HP laptop (Intel Core i9 and Nvidia RTX4000 GPU, 64GB memory and 2TB storage). If these are found to be insufficient, access to The University of Queensland's High-Performance Computing resources may be required. A request for access has been made through the project's academic supervisors but has not been approved at the time of writing this proposal. If access is not granted, the project will be limited to using smaller datasets and simpler models that can run on available hardware. This may impact the quality of the results. Software selection is independent of the computer hardware resources and will be discussed below.

## Computer software resource requirements

Python is the primary programming language used for this project @vanrossumPython3Reference2009. Python is widely used in the astronomy, data science and machine learning communities and there are a large number of open source libraries available for data processing, machine learning and visualisation @robitailleAstropyCommunityPython2013, @raschkaMachineLearningPython2020.  The primary libraries that will be used include Polars for data processing, Scikit-learn for traditional machine learning methods such as nearest neighbour or random forest algorithms, PyTorch for deep learning methods and Seaborn for visualisation @polarsdevelopersPolarsBlazinglyFast2025, @pedregosaScikitlearnMachineLearning2011, @paszkePyTorchImperativeStyle2019, @waskomSeabornStatisticalData2021 .  Jupyter Notebooks will be used as the primary development environment [@kluyverJupyterNotebooksPublishing2016]. All of these Python libraries are open source and freely available. Github will be used for managing the source code for the project via a dedicated repository @githubGitHub2025. Software will also be required to document the project's development and outcomes. It is important to be able to generate reports and presentations so that the documents can be easily reproduced by any one with access to the project's materials. 

The ability of anyone to easily replicate results and reports is embodied in the concept of Reproducible Research and its principles will be followed in this project @alstonBeginnersGuideConducting2021.  Quarto is a open source tool for creating documents, reports and presentations that can integrate code, results (including images and tables generated by code) and narrative text @allaireQuarto2025.  Quarto has been used for this document and will be used for all subsequent reports and presentations. The combination of Jupyter Notebooks, Python, Github and Quarto provides a powerful and flexible environment for developing, documenting and sharing the results of this project. All of the software and reports must be backed up to ensure that no work is lost.

Backing up work is a critical activity to ensure that work is not lost and that project milestones are able to be met in the event that a computer fails or important documents are deleted or corrupted.  I have a comprehensive backup strategy on my Apple computers that stores local Apple Time Machine backups on network attached storage (NAS) at my home and offsite backups to Backblaze and Amazon S3 Deep Glacier cloud services. Time Machine and Backblaze store all changes to files allowing recovery of previous versions if necessary and data can be restored immediately.  Amazon S3 Deep Glacier is designed for long term archival of data and is not suitable for frequent restores but is a very low cost solution for long term storage of important data.  The combination of these three backup methods plus source code control via Github provides a robust and reliable backup solution for the project.    

# Schedule
Outline the expected time frames for the semester perhaps include an Gantt chart.

![Project Gantt chart](images/galaxy_classification_timeline.pdf){#fig-project_gantt_chart width=150%}

# Costs
The high performance galaxy classification project does not incur any direct financial costs.  The datasets are all freely available and open source software will be used for the data processing, machine learning, evaluation and reporting.  The computer hardware required for the project is already owned by me and available for this project. The software used for cloud backups is already licensed for my own personal use and the incremental storage costs for this project are less than 20 cents per month.  I am happy to pay the backup costs for my own peace of mind.  The main cost for galaxy classification project is my time.  I am budgeting approximately 130 hours during Semester 1 2026 to complete this project.  I am budgeting approximately 1 hour per week for supervision and advice plus an additional 5 hours for marking of assessment that may be required in DATA7903. 

# Qualifications
I have a Bachelor of Engineering (Chemical) Hons Class 1 and Bachelor of Science (Computer Science) from the University of Queensland.  I have over 25 years of experience in the oil and gas industry as a process engineer.  For much of that time I was working with large industrial datasets but lacked the tools to handle them effectively.  In 2016 I started working on a PhD project focused on energy optimisation within the steel industry.  While working on the PhD I taught myself R and also a lot of data science and machine learning skills.  Unfortunately, I was unable to complete the PhD.  Between 2021 and 2025 I worked on a new Hydrocarbon Accounting business process for Origin Energy's coal seam gas division.  This involved significant data wrangling and analysis from a variety of internal data sources to not only identify data quality issues but also define the requirements for a new system to support the business process.  Many of the datasets I was working with had millions of rows and hundreds of columns.  This is my second semester of the Master of Data Science degree and I expect to complete the program at the end of Semester 1 2026.  I achieved four sevens in my first semester courses.  I've had a long interest in astronomy and cosmology and was an active member of a school Astronomy club.  In 2017 I helped a friend to fit out an observatory for Brisbane Girls Grammar School with three telescopes including professional grade cameras and telescopes.  I have the necessary skills and experience to complete this project successfully. 

# References

::: {#refs}
:::

# Appendices


```{python}

```
